{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidrectional LSTM with pre-trained Twitter Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cliche used an ensemble of bidirectional LSTMs along with CNNs to produce state of the art results in Twitter sentiment analysis. He trains initial word embeddings on a large, unlabled corpus of Twitter data using a neural language model. We will instead be using Stanford's pre-trained Glove word embeddings that were specifically trained on Twitter data. Since our training data is not very large, we anticipate that using these pre-trained word embeddings will result in an increase in performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob, os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper methods for reading tweets and cleaning them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv(file_path):\n",
    "    df = pd.read_table(file_path)\n",
    "    return df\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "# code inspired from https://www.kaggle.com/rahulvv/bidirectional-lstm-glove200d\n",
    "\n",
    "\n",
    "def remove_urls(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "  \n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def split_text(text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "def lower(text):\n",
    "    text = [word.lower() for word in text]\n",
    "    return str(text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', str(text))\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    pattern = re.compile(r'\\b('+r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "    text = pattern.sub(' ', text)\n",
    "    return text\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    text = lemmatizer.lemmatize(text)\n",
    "    return text\n",
    "\n",
    "def clean_tweet(text):\n",
    "    t0 = remove_urls(text)\n",
    "    t1 = remove_html(t0)\n",
    "    t2 = split_text(t1)\n",
    "    t3 = lower(t2)\n",
    "    t4 = remove_punct(t3)\n",
    "    t5 = remove_stopwords(t4)\n",
    "    t6 = lemmatize_words(t5)\n",
    "    return t6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.DataFrame(columns=['tweet', 'sentiment','NA'])\n",
    "df_test = pd.DataFrame(columns=['tweet', 'sentiment','NA'])\n",
    "\n",
    "for file in glob.glob(\"*.tsv\"):\n",
    "        if 'final_test' in file:\n",
    "            df_test_cur = read_tsv(file)\n",
    "            df_test = pd.concat([df_test, df_test_cur])\n",
    "        else:\n",
    "            df_train_cur = read_tsv(file)\n",
    "            tweet_df = pd.concat([tweet_df, df_train_cur])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  tweet sentiment\n",
      "0     05 Beat it - Michael Jackson - Thriller (25th ...   neutral\n",
      "1     Jay Z joins Instagram with nostalgic tribute t...  positive\n",
      "2     Michael Jackson: Bad 25th Anniversary Edition ...   neutral\n",
      "3     I liked a @YouTube video http://t.co/AaR3pjp2P...  positive\n",
      "4     18th anniv of Princess Diana's death. I still ...  positive\n",
      "...                                                 ...       ...\n",
      "1137                     Maybe it was - his - fantasy ?  positive\n",
      "1138  It was ok , but they always just seem so nervo...  negative\n",
      "1139  It is streamable from YepRoc -- matter of fact...  positive\n",
      "1140  comment telling me who you are , or how you fo...  positive\n",
      "1141  im on myspace ... ill try and find you and add...   neutral\n",
      "\n",
      "[53368 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(tweet_df[['tweet', 'sentiment']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   tweet sentiment\n",
      "0      #ArianaGrande Ari By Ariana Grande 80% Full ht...   neutral\n",
      "1      Ariana Grande KIIS FM Yours Truly CD listening...  positive\n",
      "2      Ariana Grande White House Easter Egg Roll in W...  positive\n",
      "3      #CD #Musics Ariana Grande Sweet Like Candy 3.4...  positive\n",
      "4      SIDE TO SIDE üòò @arianagrande #sidetoside #aria...   neutral\n",
      "...                                                  ...       ...\n",
      "11901  @dansen17 update: Zac Efron kissing a puppy ht...  positive\n",
      "11902  #zac efron sex pic skins michelle sex https://...   neutral\n",
      "11903  First Look at Neighbors 2 with Zac Efron Shirt...   neutral\n",
      "11904  zac efron poses nude #lovely libra porn https:...   neutral\n",
      "11905  #Fashion #Style The Paperboy (NEW Blu-ray Disc...   neutral\n",
      "\n",
      "[11906 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_test[['tweet', 'sentiment']] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Glove word embeddings into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing train lables\n",
    "tweet_df.loc[tweet_df.sentiment == \"positive\", \"sentiment\"] = 2\n",
    "tweet_df.loc[tweet_df.sentiment == \"neutral\", \"sentiment\"] = 1\n",
    "tweet_df.loc[tweet_df.sentiment == \"negative\", \"sentiment\"] = 0\n",
    "\n",
    "labels = tweet_df[\"sentiment\"].tolist()\n",
    "labels = [ int(x) for x in labels ]\n",
    "\n",
    "#preparing test labels\n",
    "df_test.loc[df_test.sentiment == \"positive\", \"sentiment\"] = 2\n",
    "df_test.loc[df_test.sentiment == \"neutral\", \"sentiment\"] = 1\n",
    "df_test.loc[df_test.sentiment == \"negative\", \"sentiment\"] = 0\n",
    "\n",
    "labels_test = df_test[\"sentiment\"].tolist()\n",
    "labels_test = [ int(x) for x in labels_test ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting tweets and labels into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = tweet_df.tweet.values\n",
    "y_train_orig = tweet_df.sentiment.values\n",
    "test_tweets = df_test.tweet.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(y_train_orig)\n",
    "\n",
    "clean_training_tweets = []\n",
    "for i in range(len(train_tweets)):\n",
    "    data = clean_tweet(train_tweets[i])\n",
    "    clean_training_tweets.append(data)\n",
    "\n",
    "clean_testing_tweets = []\n",
    "for i in range(len(test_tweets)):\n",
    "    data = clean_tweet(test_tweets[i])\n",
    "    clean_testing_tweets.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the tweets after cleaning them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' beat  michael jackson  thriller th anniversary edition hd', 'jay z joins instagram  nostalgic tribute  michael jackson jay z apparently joined instagram  saturday  ', 'michael jackson bad th anniversary edition picture vinyl  unique picture disc vinyl includes  original ', ' liked  youtube video one direction singing man   mirror  michael jackson  atlanta ga june ', 'th anniv  princess dianas death  still want  believe   living   private island away   public  michael jackson', 'oridaganjazz  st time  heard michael jackson sing   honolulu hawaii   restaurant  radio   abc    loved  ', 'michael jackson appeared  saturday    th place   top  miamis trends trndnl', '  old enough  remember michael jackson attending  grammys  brooke shields  webster sat   lap   show', 'etbowser  u enjoy  nd rate michael jackson bit honest ques like  cant feel face song  god   obvious  want mj ', ' weeknd   closest thing  may get  michael jackson   long timeespecially since  damn near mimics everything']\n",
      "['arianagrande ari  ariana grande  full singer actress', 'ariana grande kiis fm  truly cd listening party  burbank arianagrande', 'ariana grande white house easter egg roll  washington arianagrande', 'cd musics ariana grande sweet like candy  oz  ml sealed  box  authenic new', 'side  side üòò arianagrande sidetoside arianagrande musically comunidadgay lgbtüåà lotb‚Ä¶', 'hairspray live previews   macys thanksgiving day parade arianagrande televisionnbc', 'lindsaylohan  ‚Äòfeeling thankful‚Äô  blasting arianagrande  wearing ‚Äòtoomuch‚Ä¶', ' hate    love  songs dammit arianagrande', 'ariana grande „Äêright  ft big sean„Äë„Ç¢„É™„Ç¢„Éä arianagrande', ' one would  prefer  listen    whole day üòçü§òüèº  could never choose arianagrande intoyou sidetoside songs poll']\n"
     ]
    }
   ],
   "source": [
    "print(clean_training_tweets[:10])\n",
    "print(clean_testing_tweets[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "with open(os.path.join('../glove/glove.twitter.27B.200d.txt'), encoding = \"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "print('Found %s word vectors.' % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 67101 unique words in train tweets.\n"
     ]
    }
   ],
   "source": [
    "# converting tweets to integer sequences \n",
    "tokenizer = Tokenizer(num_words= 20000, oov_token= 'OOV')\n",
    "tokenizer.fit_on_texts(clean_training_tweets)\n",
    "train_tweet_sequences = tokenizer.texts_to_sequences(clean_training_tweets)\n",
    "word_index_train = tokenizer.word_index\n",
    "print('Found %s unique words in train tweets.' % len(word_index_train))\n",
    "X_train = pad_sequences(sequences=train_tweet_sequences, maxlen=32, padding= 'post', truncating='post')\n",
    "\n",
    "\n",
    "test_tweet_sequences = tokenizer.texts_to_sequences(clean_testing_tweets)\n",
    "X_test = pad_sequences(sequences= test_tweet_sequences, maxlen=32, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train tensor:  (53368, 32)\n",
      "Shape of X test:  (11906, 32)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X train tensor: ', X_train.shape)\n",
    "print('Shape of X test: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(20000, len(word_index_train)+1)\n",
    "embedding_matrix = np.zeros((num_words, 200))\n",
    "\n",
    "embeddings = []\n",
    "for word, i in word_index_train.items():\n",
    "    if i<20000:\n",
    "        embeddings = word2vec.get(word)\n",
    "        if embeddings is not None:\n",
    "            embedding_matrix[i] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(input_dim=num_words,output_dim = 200, weights=[embedding_matrix], input_length=32,trainable=False))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100, return_sequences=True)))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.01), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 32, 200)           4000000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 32, 200)           240800    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 32, 64)            59648     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 6147      \n",
      "=================================================================\n",
      "Total params: 4,306,595\n",
      "Trainable params: 306,595\n",
      "Non-trainable params: 4,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/15\n",
      "53368/53368 [==============================] - 103s 2ms/sample - loss: 0.7818 - acc: 0.6416\n",
      "Epoch 2/15\n",
      "53368/53368 [==============================] - 108s 2ms/sample - loss: 0.6944 - acc: 0.6876\n",
      "Epoch 3/15\n",
      "53368/53368 [==============================] - 100s 2ms/sample - loss: 0.6321 - acc: 0.7211\n",
      "Epoch 4/15\n",
      "53368/53368 [==============================] - 99s 2ms/sample - loss: 0.5661 - acc: 0.7535\n",
      "Epoch 5/15\n",
      "53368/53368 [==============================] - 95s 2ms/sample - loss: 0.4978 - acc: 0.7854\n",
      "Epoch 6/15\n",
      "53368/53368 [==============================] - 92s 2ms/sample - loss: 0.4374 - acc: 0.8170\n",
      "Epoch 7/15\n",
      "53368/53368 [==============================] - 91s 2ms/sample - loss: 0.3940 - acc: 0.8358\n",
      "Epoch 8/15\n",
      "53368/53368 [==============================] - 92s 2ms/sample - loss: 0.3537 - acc: 0.8544\n",
      "Epoch 9/15\n",
      "53368/53368 [==============================] - 92s 2ms/sample - loss: 0.3260 - acc: 0.8683\n",
      "Epoch 10/15\n",
      "53368/53368 [==============================] - 92s 2ms/sample - loss: 0.3010 - acc: 0.8809\n",
      "Epoch 11/15\n",
      "53368/53368 [==============================] - 92s 2ms/sample - loss: 0.2785 - acc: 0.8896\n",
      "Epoch 12/15\n",
      "53368/53368 [==============================] - 92s 2ms/sample - loss: 0.2758 - acc: 0.8918\n",
      "Epoch 13/15\n",
      "53368/53368 [==============================] - 92s 2ms/sample - loss: 0.2516 - acc: 0.9013\n",
      "Epoch 14/15\n",
      "53368/53368 [==============================] - 92s 2ms/sample - loss: 0.2503 - acc: 0.9038\n",
      "Epoch 15/15\n",
      "53368/53368 [==============================] - 94s 2ms/sample - loss: 0.2383 - acc: 0.9092\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train, train_labels, batch_size=128, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_p = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (np.round(pred_p)).astype(int)\n",
    "final_pred = []\n",
    "for sample in pred:\n",
    "    pred_label = sample.argmax()\n",
    "    final_pred.append(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11906/11906 [==============================] - 10s 830us/sample - loss: 1.6071 - acc: 0.5879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6070550479709491, 0.58793885]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_binary = to_categorical(labels_test)\n",
    "model.evaluate(x = X_test, y =y_binary )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.61      0.59      3811\n",
      "           1       0.62      0.58      0.60      5743\n",
      "           2       0.51      0.56      0.53      2352\n",
      "\n",
      "    accuracy                           0.58     11906\n",
      "   macro avg       0.57      0.58      0.58     11906\n",
      "weighted avg       0.59      0.58      0.58     11906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels_test, final_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling `save('my_model')` creates a SavedModel folder `my_model`.\n",
    "model.save(\"bidirectional-lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11906/11906 [==============================] - 13s 1ms/sample - loss: 1.6071 - acc: 0.5879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6070550479709491, 0.58793885]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It can be used to reconstruct the model identically.\n",
    "reconstructed_model = keras.models.load_model(\"bidirectional-lstm\")\n",
    "y_binary = to_categorical(labels_test)\n",
    "reconstructed_model.evaluate(x = X_test, y =y_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.61      0.59      3811\n",
      "           1       0.62      0.58      0.60      5743\n",
      "           2       0.51      0.56      0.53      2352\n",
      "\n",
      "    accuracy                           0.58     11906\n",
      "   macro avg       0.57      0.58      0.58     11906\n",
      "weighted avg       0.59      0.58      0.58     11906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "pred_p = reconstructed_model.predict(X_test)\n",
    "\n",
    "pred = (np.round(pred_p)).astype(int)\n",
    "final_pred = []\n",
    "for sample in pred:\n",
    "    pred_label = sample.argmax()\n",
    "    final_pred.append(pred_label)\n",
    "    \n",
    "print(classification_report(labels_test, final_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:neural_networks] *",
   "language": "python",
   "name": "conda-env-neural_networks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
