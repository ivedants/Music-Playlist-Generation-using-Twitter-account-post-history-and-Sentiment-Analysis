{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutinal Neural Network for Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Kim suggested in his 2014 paper, Convolutional neural networks for sentence classification, a simple CNN with a single convolutional layer can produce excellent results for sentence classification. Therefore, we decided to test their effectiveness for our task. In this notebook, we will train our own word embeddings on the training dataset. The embedding layer is then followed by a convolution layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/shivaomrani/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for reading training and test datasets into dataframes and for pre-processing tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# inspired from https://www.kaggle.com/rahulvv/bidirectional-lstm-glove200d\n",
    "\n",
    "def read_tsv(file_path):\n",
    "    df = pd.read_table(file_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_urls(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "  \n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "\n",
    "def split_text(text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "def lower(text):\n",
    "    text = [word.lower() for word in text]\n",
    "    return str(text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', str(text))\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    pattern = re.compile(r'\\b('+r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "    text = pattern.sub(' ', text)\n",
    "    return text\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    text = lemmatizer.lemmatize(text)\n",
    "    return text\n",
    "\n",
    "def clean_tweet(text):\n",
    "    t0 = remove_urls(text)\n",
    "    t1 = remove_html(t0)\n",
    "    t2 = split_text(t1)\n",
    "    t3 = lower(t2)\n",
    "    t4 = remove_punct(t3)\n",
    "    t5 = remove_stopwords(t4)\n",
    "    t6 = lemmatize_words(t5)\n",
    "    return t6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.DataFrame(columns=['tweet', 'sentiment','NA'])\n",
    "df_test = pd.DataFrame(columns=['tweet', 'sentiment','NA'])\n",
    "\n",
    "for file in glob.glob(\"*.tsv\"):\n",
    "        if 'final_test' in file:\n",
    "            df_test_cur = read_tsv(file)\n",
    "            df_test = pd.concat([df_test, df_test_cur])\n",
    "        else:\n",
    "            df_train_cur = read_tsv(file)\n",
    "            tweet_df = pd.concat([tweet_df, df_train_cur])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will print some of the training and test datasets tweets to get an idea of how they look before pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  tweet sentiment\n",
      "0     05 Beat it - Michael Jackson - Thriller (25th ...   neutral\n",
      "1     Jay Z joins Instagram with nostalgic tribute t...  positive\n",
      "2     Michael Jackson: Bad 25th Anniversary Edition ...   neutral\n",
      "3     I liked a @YouTube video http://t.co/AaR3pjp2P...  positive\n",
      "4     18th anniv of Princess Diana's death. I still ...  positive\n",
      "...                                                 ...       ...\n",
      "1137                     Maybe it was - his - fantasy ?  positive\n",
      "1138  It was ok , but they always just seem so nervo...  negative\n",
      "1139  It is streamable from YepRoc -- matter of fact...  positive\n",
      "1140  comment telling me who you are , or how you fo...  positive\n",
      "1141  im on myspace ... ill try and find you and add...   neutral\n",
      "\n",
      "[53368 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(tweet_df[['tweet', 'sentiment']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   tweet sentiment\n",
      "0      #ArianaGrande Ari By Ariana Grande 80% Full ht...   neutral\n",
      "1      Ariana Grande KIIS FM Yours Truly CD listening...  positive\n",
      "2      Ariana Grande White House Easter Egg Roll in W...  positive\n",
      "3      #CD #Musics Ariana Grande Sweet Like Candy 3.4...  positive\n",
      "4      SIDE TO SIDE üòò @arianagrande #sidetoside #aria...   neutral\n",
      "...                                                  ...       ...\n",
      "11901  @dansen17 update: Zac Efron kissing a puppy ht...  positive\n",
      "11902  #zac efron sex pic skins michelle sex https://...   neutral\n",
      "11903  First Look at Neighbors 2 with Zac Efron Shirt...   neutral\n",
      "11904  zac efron poses nude #lovely libra porn https:...   neutral\n",
      "11905  #Fashion #Style The Paperboy (NEW Blu-ray Disc...   neutral\n",
      "\n",
      "[11906 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_test[['tweet', 'sentiment']] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then pre-process the tweets and select a subset of the tweets so that they work with our specified number of batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original length of train tweets:  53368\n",
      "original length of test tweets:  11906\n",
      "custom length of train tweets:  53000\n",
      "custom length of test tweets:  11900\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "clean_tweets = []\n",
    "for tweet in tweet_df['tweet']:\n",
    "    clean_tweets.append(clean_tweet(tweet))\n",
    "\n",
    "clean_tweets_1 = clean_tweets[:53000]   \n",
    "\n",
    "\n",
    "\n",
    "clean_tweets_test = []\n",
    "for tweet in df_test['tweet']:\n",
    "    clean_tweets_test.append(clean_tweet(tweet))\n",
    "\n",
    "clean_tweets_test_1 = clean_tweets_test[:11900] \n",
    "\n",
    "\n",
    "print(\"original length of train tweets: \", len(clean_tweets))\n",
    "print(\"original length of test tweets: \", len(clean_tweets_test))\n",
    "\n",
    "print(\"custom length of train tweets: \", len(clean_tweets_1))\n",
    "print(\"custom length of test tweets: \", len(clean_tweets_test_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' beat  michael jackson  thriller th anniversary edition hd', 'jay z joins instagram  nostalgic tribute  michael jackson jay z apparently joined instagram  saturday  ', 'michael jackson bad th anniversary edition picture vinyl  unique picture disc vinyl includes  original ', ' liked  youtube video one direction singing man   mirror  michael jackson  atlanta ga june ', 'th anniv  princess dianas death  still want  believe   living   private island away   public  michael jackson', 'oridaganjazz  st time  heard michael jackson sing   honolulu hawaii   restaurant  radio   abc    loved  ', 'michael jackson appeared  saturday    th place   top  miamis trends trndnl', '  old enough  remember michael jackson attending  grammys  brooke shields  webster sat   lap   show', 'etbowser  u enjoy  nd rate michael jackson bit honest ques like  cant feel face song  god   obvious  want mj ', ' weeknd   closest thing  may get  michael jackson   long timeespecially since  damn near mimics everything']\n",
      "['arianagrande ari  ariana grande  full singer actress', 'ariana grande kiis fm  truly cd listening party  burbank arianagrande', 'ariana grande white house easter egg roll  washington arianagrande', 'cd musics ariana grande sweet like candy  oz  ml sealed  box  authenic new', 'side  side üòò arianagrande sidetoside arianagrande musically comunidadgay lgbtüåà lotb‚Ä¶', 'hairspray live previews   macys thanksgiving day parade arianagrande televisionnbc', 'lindsaylohan  ‚Äòfeeling thankful‚Äô  blasting arianagrande  wearing ‚Äòtoomuch‚Ä¶', ' hate    love  songs dammit arianagrande', 'ariana grande „Äêright  ft big sean„Äë„Ç¢„É™„Ç¢„Éä arianagrande', ' one would  prefer  listen    whole day üòçü§òüèº  could never choose arianagrande intoyou sidetoside songs poll']\n"
     ]
    }
   ],
   "source": [
    "print(clean_tweets_1[:10])\n",
    "print(clean_tweets_test_1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will convert our data to integer sequences (where each tweet is represented as a sequence of numbers and the numbers are the index of the words in the whole vocabulary). This prepared our data to be fed into the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 66956 unique words in train tweets.\n"
     ]
    }
   ],
   "source": [
    "# converting tweets to integer sequences \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_tweets_1)\n",
    "tweet_sequences = tokenizer.texts_to_sequences(clean_tweets_1)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique words in train tweets.' % len(word_index))\n",
    "\n",
    "\n",
    "# converting tweets to integer sequences \n",
    "tweet_sequences_test = tokenizer.texts_to_sequences(clean_tweets_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the training data into train and validation so we have a better idea of how our model is doing on the validation set in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of x train  42400\n",
      "length of y train  42400\n",
      "length of x val:  10600\n",
      "length of y val:  10600\n"
     ]
    }
   ],
   "source": [
    "#preparing train lables\n",
    "tweet_df.loc[tweet_df.sentiment == \"positive\", \"sentiment\"] = 2\n",
    "tweet_df.loc[tweet_df.sentiment == \"neutral\", \"sentiment\"] = 1\n",
    "tweet_df.loc[tweet_df.sentiment == \"negative\", \"sentiment\"] = 0\n",
    "\n",
    "labels = tweet_df[\"sentiment\"].tolist()\n",
    "labels = [ int(x) for x in labels ]\n",
    "\n",
    "labels_1 = labels[:53000]\n",
    "\n",
    "# Split the train set into train and validation set\n",
    "x_train, x_val, y_train, y_val = train_test_split(tweet_sequences, labels_1, test_size = 0.2, random_state=17)\n",
    "\n",
    "#preparing test labels\n",
    "df_test.loc[df_test.sentiment == \"positive\", \"sentiment\"] = 2\n",
    "df_test.loc[df_test.sentiment == \"neutral\", \"sentiment\"] = 1\n",
    "df_test.loc[df_test.sentiment == \"negative\", \"sentiment\"] = 0\n",
    "\n",
    "labels_test = df_test[\"sentiment\"].tolist()\n",
    "labels_test = [ int(x) for x in labels_test ]\n",
    "\n",
    "labels_test_1 = labels_test[:11900]\n",
    "\n",
    "print(\"length of x train \", len(x_train))\n",
    "print(\"length of y train \", len(y_train))\n",
    "print(\"length of x val: \", len(x_val))\n",
    "print(\"length of y val: \", len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further prepare our data to be fed into the embedding layer, we convert the list of integer sequences to tensors and we will pad 0s and the end of each sequence so that our samples are all of equal size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Put into tensors\n",
    "x_train = [torch.tensor(x) for x in x_train]\n",
    "X_train = nn.utils.rnn.pad_sequence(x_train, batch_first=True, padding_value=0).long()\n",
    "X_train = X_train.view(-1, batch_size, X_train.shape[1])\n",
    "\n",
    "x_val = [torch.tensor(x) for x in x_val]\n",
    "X_val = nn.utils.rnn.pad_sequence(x_val, batch_first=True, padding_value=0).long()\n",
    "X_val = X_val.view(-1, batch_size, X_val.shape[1])\n",
    "\n",
    "\n",
    "x_test = [torch.tensor(x) for x in tweet_sequences_test]\n",
    "X_test = nn.utils.rnn.pad_sequence(x_test, batch_first=True, padding_value=0,).long()\n",
    "X_test = X_test.view(-1, batch_size, X_test.shape[1])\n",
    "\n",
    "y_train = torch.tensor(y_train).view(-1, batch_size)\n",
    "y_val = torch.tensor(y_val)\n",
    "y_test = torch.tensor(labels_test_1)\n",
    "\n",
    "\n",
    "# Apply the same length of X_train on X_val and X_test\n",
    "len_voc = int((X_train.max()+1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X train  torch.Size([424, 100, 555])\n",
      "shape of y train  torch.Size([424, 100])\n",
      "shape of X val  torch.Size([106, 100, 626])\n",
      "shape of y val  torch.Size([10600])\n",
      "shape of X test  torch.Size([119, 100, 653])\n",
      "shape of y test  torch.Size([11900])\n"
     ]
    }
   ],
   "source": [
    "# double checking the shapes\n",
    "\n",
    "print(\"shape of X train \", X_train.shape)\n",
    "print(\"shape of y train \", y_train.shape)\n",
    "\n",
    "print(\"shape of X val \", X_val.shape)\n",
    "print(\"shape of y val \", y_val.shape)\n",
    "\n",
    "print(\"shape of X test \", X_test.shape)\n",
    "print(\"shape of y test \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch model layout inspired from https://towardsdatascience.com/convolutional-neural-network-in-natural-language-processing-96d67f91275c\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# The shape of our embeddings will be the size of words in the vocab, followed by the dimension of embedding (50)\n",
    "embeddings = nn.Embedding(len_voc, 50)\n",
    "\n",
    "# Build CNN model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.embeddings = nn.Embedding(len_voc, 50)\n",
    "        #creating a convolution with 1 inout channel, 100 output channels, and 3x 50 kernel size\n",
    "        self.cnn = nn.Conv2d(1, 100, (3, 50))\n",
    "        \n",
    "        # we will have three neurons at the last layer before the activation function for our three classes.\n",
    "        self.clf = nn.Linear(100, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        # Add an extra dimension for CNN\n",
    "        x = x.unsqueeze(1)\n",
    "        # Apply CNN\n",
    "        x = self.cnn(x)\n",
    "        # Choose the maximum value of each filter and delete the extra dimension\n",
    "        x = x.max(2)[0].squeeze(2)\n",
    "        # Choose the most important features for the classification\n",
    "        x = F.relu(x) \n",
    "        #  Apply linear nn for classification\n",
    "        x = self.clf(x)\n",
    "        return x\n",
    "\n",
    "  \n",
    "model = Model()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterio  = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "\n",
    "# Function for evaluating returns f1 followed by accuracy\n",
    "def get_f1(X, y_real):\n",
    "  y_pred = []\n",
    "  for x in X:\n",
    "      # Choose the value (class label) with higher probability\n",
    "      predicted = model(x).argmax(1).cpu().detach()\n",
    "      y_pred.append(predicted)\n",
    "    \n",
    "  y_pred_torch = torch.cat(y_pred)\n",
    "  acc = metrics.accuracy_score(y_true=y_real, y_pred= y_pred_torch)\n",
    "  return metrics.f1_score(y_true=y_real, y_pred=y_pred_torch, average='micro'), acc, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Batch: 0 \t Loss: 0.9495213032 \t F1_val: 0.4471698113 \t Accuracy: 0.4471698113\n",
      "Epoch: 0 \t Batch: 200 \t Loss: 0.9902819991 \t F1_val: 0.5126415094 \t Accuracy: 0.5126415094\n",
      "Epoch: 0 \t Batch: 400 \t Loss: 0.9411697388 \t F1_val: 0.5493396226 \t Accuracy: 0.5493396226\n",
      "Epoch: 1 \t Batch: 0 \t Loss: 0.9342650771 \t F1_val: 0.5516981132 \t Accuracy: 0.5516981132\n",
      "Epoch: 1 \t Batch: 200 \t Loss: 0.8722500801 \t F1_val: 0.5641509434 \t Accuracy: 0.5641509434\n",
      "Epoch: 1 \t Batch: 400 \t Loss: 0.8265760541 \t F1_val: 0.5750000000 \t Accuracy: 0.5750000000\n",
      "Epoch: 2 \t Batch: 0 \t Loss: 0.8226707578 \t F1_val: 0.5768867925 \t Accuracy: 0.5768867925\n",
      "Epoch: 2 \t Batch: 200 \t Loss: 0.7616884112 \t F1_val: 0.5855660377 \t Accuracy: 0.5855660377\n",
      "Epoch: 2 \t Batch: 400 \t Loss: 0.7139935493 \t F1_val: 0.5846226415 \t Accuracy: 0.5846226415\n",
      "Epoch: 3 \t Batch: 0 \t Loss: 0.7108892798 \t F1_val: 0.5871698113 \t Accuracy: 0.5871698113\n",
      "Epoch: 3 \t Batch: 200 \t Loss: 0.6455029249 \t F1_val: 0.5943396226 \t Accuracy: 0.5943396226\n",
      "Epoch: 3 \t Batch: 400 \t Loss: 0.6001831293 \t F1_val: 0.5927358491 \t Accuracy: 0.5927358491\n",
      "Epoch: 4 \t Batch: 0 \t Loss: 0.5976411700 \t F1_val: 0.5944339623 \t Accuracy: 0.5944339623\n",
      "Epoch: 4 \t Batch: 200 \t Loss: 0.5271141529 \t F1_val: 0.5950943396 \t Accuracy: 0.5950943396\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-73c4c72a6f97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-fd4d2e4c7af0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Apply CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Choose the maximum value of each filter and delete the extra dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/neural_networks/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training steps\n",
    "epochs = 20\n",
    "LOSS = []\n",
    "for e in range(epochs):\n",
    "    for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "        \n",
    "        # Delete the prvious values of the gradient\n",
    "        optimizer.zero_grad()\n",
    "        x, y = x, y\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        loss = criterio(y_pred, y)\n",
    "\n",
    "        # Compute the gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # Apply the optimization method for one step\n",
    "        optimizer.step()\n",
    "        \n",
    "        LOSS.append(loss.item())\n",
    "        if i%200==0:\n",
    "            with torch.no_grad():\n",
    "                f1, acc, y_pred = get_f1(X_val, y_val)\n",
    "            print('Epoch: %d \\t Batch: %d \\t Loss: %.10f \\t F1_val: %.10f \\t Accuracy: %.10f'%(e,i, torch.tensor(LOSS[-100:]).mean(), f1, acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " f1 is  0.5085714285714286  accuracy is  0.5085714285714286\n"
     ]
    }
   ],
   "source": [
    "f1, acc, y_pred = get_f1(X_test, y_test)\n",
    "print(\" f1 is \", f1, \" accuracy is \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11900\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.28      0.37      3811\n",
      "           1       0.55      0.66      0.60      5739\n",
      "           2       0.39      0.52      0.45      2350\n",
      "\n",
      "    accuracy                           0.51     11900\n",
      "   macro avg       0.50      0.48      0.47     11900\n",
      "weighted avg       0.52      0.51      0.49     11900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predicted_labels_list = []\n",
    "\n",
    "flat_list = [item for sub in y_pred for item in sub]\n",
    "\n",
    "for elem in flat_list:\n",
    "    predicted_labels_list.append(elem.tolist())\n",
    "\n",
    "print(len(predicted_labels_list))\n",
    "print(classification_report(labels_test[:11900], predicted_labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:neural_networks] *",
   "language": "python",
   "name": "conda-env-neural_networks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
